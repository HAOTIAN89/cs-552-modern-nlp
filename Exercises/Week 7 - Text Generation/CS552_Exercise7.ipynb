{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-GlgO5WBOYeF"
      },
      "source": [
        "# üìö  Exercise Session - Week 7\n",
        "**Main Topics**: Generation, Decoding, Beam-search, Sampling, NLG Eval Metrics\n",
        "\n",
        "\n",
        "Welcome to Week 7 of CS-552 Modern NLP's exercise sessions!\n",
        "The aim of this exercise is to get yourself acquainted with generation algorithms that are used with language models. \n",
        "We demonstrate the different generation algorithms with a fine-tuned version of the [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf) model.\n",
        "\n",
        "1. [Story Generator Model:](#1-story-generator-model)\n",
        "    - Load a GPT-2 model that's only been pretrained\n",
        "    - Load a GPT-2 model finetuned on the BookCorpus dataset\n",
        "2. [Decoding Algorithms](#2-decoding-algorithms)\n",
        "    1. [Greedy Search](#21-greedy-search)\n",
        "    2. [Beam Search](#22-beam-search)\n",
        "    3. [Decoding Takeaways](#23-decoding-takeaways)\n",
        "3. [Sampling Algorithms:](#3-nucleus-sampling)\n",
        "    1. [Random Sampling](#31-random-sampling)\n",
        "    2. [Top-$p$ Sampling](#32-top--nucleus)\n",
        "    3. [Sampling Takeaways](#33-sampling-takeaways)\n",
        "4. [Automatic NLG Evaluation Metrics - the curious case of BLEU:](#4-automatic-nlg-evaluation-metrics---the-curious-case-of-bleu)\n",
        "    1. [Pitfall 1: Correct Paraphrases](#41-pitfall-1-accurate-paraphrases)\n",
        "    2. [Pitfall 2: Senseless Repetitions](#42-pitfall-2-senseless-repetitions)\n",
        "    3. [Other Problems](#43-other-problems-with-bleu)\n",
        "5. [Conclusion](#5-conclusion)\n",
        "\n",
        "> **By the end of the session you will be able to:**\n",
        "> - ‚úÖ  Generate stories with Language Models\n",
        "> - ‚úÖ  Use decoder algorithms such as greedy and beam search with huggingface and understand their pitfalls\n",
        "> - ‚úÖ  Use sampling algorithms such as top-k and top-k sampling with huggingface and understand their pitfalls \n",
        "> - ‚úÖ  Understand the BLEU NLG evaluation metric and recognize its pitfalls"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"padding:8px 0 8px 15px;border-left:3px solid orange;\">\n",
        "\n",
        "### ‚ö†Ô∏è **Note: Cells are pre-run. You don't need to execute any cells, but you are welcome to. It will download the finetuned GPT2 model which might take around 1.5-3 minutes depending on your internet connection.** ‚ö†Ô∏è\n",
        "So if you would like to, you can skip the setup section!\n",
        "\n",
        "### Setup\n",
        "- Colab: If you are using Colab (which has Python version 3.9.16) then download the following packages by creating a code cell below:\n",
        "```\n",
        "!pip install -U ipykernel\n",
        "!pip install -U ipywidgets\n",
        "!pip install -U setuptools \n",
        "!pip install -U wheel\n",
        "!pip install -U transformers datasets evaluate\n",
        "!pip install -U torch torchmetrics torchvision\n",
        "```\n",
        "\n",
        "- Local: If you are not using Colab, create an environment with Python version 3.9.16 and then install the packages in this exercise session's `requirements.txt` (not the one from the beginning of the course!) with `pip install -r requirements.txt`.\n",
        "\n",
        "### Acknowledgements\n",
        "We are very grateful to [Patrick von Platen](https://huggingface.co/patrickvonplaten) for writing a [blogpost on different generation techniques](https://huggingface.co/blog/how-to-generate). Our notebook takes its examples, images, and wordings. \n",
        "Moreover, we would like to thank [Google Cloud Translation's evaluation documentation](https://cloud.google.com/translate/automl/docs/evaluate#:~:text=BLEU%20(BiLingual%20Evaluation%20Understudy)%20is,of%20high%20quality%20reference%20translations), which we have used for the evaluation section. We encourage you to check these resources out!\n",
        "\n",
        "The model we primarily use is the 125M parameter [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf). The [finetuned story generation version on huggingface](https://huggingface.co/pranavpsv/genre-story-generator-v2) was finetuned using the [BookCorpus dataset](https://paperswithcode.com/dataset/bookcorpus) by getting the different genres per book and prefixing the inputs with the relevant genre.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Story Generator Model\n",
        "\n",
        "First, we import the pretrained GPT-2 and a finetuned version of it with [huggingface](https://huggingface.co/)'s [transformers package](https://huggingface.co/docs/transformers/index). The latter is fine-tuned on sample of BookCorpus dataset for short story generation. This model has been fine-tuned to first take the `<BOS>` token and then a token that depicts the genre we desire to generate such as `<adventure>`. Possible genres this model handles and their respective tokens are:\n",
        "\n",
        "- Romance `<romance>`\n",
        "- Adventure `<adventure>`\n",
        "- Mystery & detective `<mystery-&-detective`\n",
        "- Fantasy `<fantasy>`\n",
        "- Humor & comedy `<humor-&-comedy>`\n",
        "- Paranormal `<paranormal>`\n",
        "- Science fiction `<science-fiction>`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "TG-dQt3NOlub"
      },
      "outputs": [],
      "source": [
        "# 1) Import the relevant packages\n",
        "import torch\n",
        "import evaluate\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "\n",
        "# 2) Load the relevant models and their respective tokenizers\n",
        "# NOTE: You could also try gpt2-medium for the pretrained model! Although generation will be a bit slower.\n",
        "pretrained_model_id = \"gpt2\"\n",
        "story_model_id = \"pranavpsv/genre-story-generator-v2\"\n",
        "#\n",
        "pretrained_tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model_id)\n",
        "story_tokenizer = GPT2Tokenizer.from_pretrained(story_model_id)\n",
        "#\n",
        "pretrained_model = GPT2LMHeadModel.from_pretrained(pretrained_model_id , pad_token_id=pretrained_tokenizer.eos_token_id) \n",
        "story_model = GPT2LMHeadModel.from_pretrained(story_model_id, pad_token_id=story_tokenizer.eos_token_id)\n",
        "#\n",
        "pretrained_model.eval()\n",
        "story_model.eval()\n",
        "\n",
        "# 3) A display function to compare the pretrained and story-finetuned GPT-2 model generations\n",
        "def display_outputs(pretrained_output, story_output, multiple_output=False):\n",
        "    if not multiple_output:\n",
        "        print(80 * '-')\n",
        "        print(\"Pretrained model output:\")\n",
        "        print(80 * '-')\n",
        "        print(pretrained_tokenizer.decode(pretrained_output[0], skip_special_tokens=True))\n",
        "        print(80 * '-')\n",
        "        print(\"Story model output:\")\n",
        "        print(80 * '-')\n",
        "        print(story_tokenizer.decode(story_output[0], skip_special_tokens=True))\n",
        "    else:\n",
        "        print(80 * '-')\n",
        "        print(\"Pretrained model output:\")\n",
        "        print(80 * '-')\n",
        "        for i, beam_output in enumerate(pretrained_output):\n",
        "            if i > 0:\n",
        "                print(20 * '-')\n",
        "            print(\"{}: {}\".format(i + 1, pretrained_tokenizer.decode(beam_output, skip_special_tokens=True)))\n",
        "        \n",
        "        print(80 * '-')\n",
        "        print(\"Story model output:\")\n",
        "        print(80 * '-')\n",
        "        for i, beam_output in enumerate(story_output):\n",
        "            if i > 0:\n",
        "                print(20 * '-')\n",
        "            print(\"{}: {}\".format(i + 1, story_tokenizer.decode(beam_output, skip_special_tokens=True)))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EYUPfoEWPxt_"
      },
      "source": [
        "## 2) Decoding Algorithms\n",
        "\n",
        "### 2.1) Greedy Search\n",
        "\n",
        "Greedy search is the first type of decoding algorithm one can imagine: we select the next token as the one with the highest probability.\n",
        "Given the following image provided by Patrick von Platen, the path that a greedy search would choose is depicted by the red path.\n",
        "\n",
        "![Greedy Search](./figs/greedy_search.png)\n",
        "\n",
        "Given the word **The**, the algorithm picks *greedily* the highest probability token **nice**, and then **woman**. Note that the generated token sequence has therefore a probability of:\n",
        "\n",
        "$0.5 \\times 0.4 = 0.2$.\n",
        "\n",
        "Now let's generate a sequence after inputting the model the sequence **\"Bella couldn't take it anymore. Edward was gone for 3 months now and\"** inspired by second movie and book of Twilight. For the story generation version we will add the prefix token `<romance>`. \n",
        "\n",
        "Note that in *transformers*, we can generate with autoregressive models simply by calling the model's *generate* function and giving it tokenized inputs as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "id": "-QnRteSLP0Hm",
        "outputId": "ee2f57d9-ba11-4f66-b19e-e6e9ddaae18d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Pretrained model output:\n",
            "--------------------------------------------------------------------------------\n",
            "Bella couldn't take it anymore. Edward was gone for 3 months now and Bella was still in the hospital.\n",
            "\n",
            "\"I'm so sorry, Bella,\" Edward said. \"I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so\n",
            "--------------------------------------------------------------------------------\n",
            "Story model output:\n",
            "--------------------------------------------------------------------------------\n",
            "<romance> Bella couldn't take it anymore. Edward was gone for 3 months now and Bella was still in love with him. Bella wanted to be with Edward but she couldn't. Bella wanted to be with Edward but she couldn't. Bella wanted to be with Edward but she couldn't. Bella wanted to be with Edward but she couldn't. Bella wanted to be with Edward but she couldn't. Bella wanted to be with Edward but she couldn't. Bella wanted to be with Edward but\n"
          ]
        }
      ],
      "source": [
        "# 1) Tokenize the text with which we condition the generation\n",
        "pretrained_input_ids = pretrained_tokenizer.encode(\"Bella couldn't take it anymore. Edward was gone for 3 months now and\", return_tensors='pt')\n",
        "story_input_ids = story_tokenizer.encode(\"<romance> Bella couldn't take it anymore. Edward was gone for 3 months now and\", return_tensors='pt')\n",
        "\n",
        "# 2) Generate text until the output length (which includes the context length) \n",
        "#    reaches max_length\n",
        "greedy_pretrained_output = pretrained_model.generate(pretrained_input_ids, max_length=100)\n",
        "greedy_story_output = story_model.generate(story_input_ids, max_length=100)\n",
        "\n",
        "# 3) Decode the output back into readable strings\n",
        "display_outputs(greedy_pretrained_output, greedy_story_output)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Q:**\n",
        "- **Given the generated output above, what is a serious problem you find about the greedy search algorithm?**\n",
        "- **Why do you think this might be happening?**\n",
        "- **Is there a difference between the pretrained model and the story finetuned model with respect to these pitfalls?**\n",
        "\n",
        "*A: TODO - your answer here!*"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2) Beam Search\n",
        "\n",
        "Instead of considering a single best next token, the beam search algorithm tries to respond to problems with greedy search by considering the next *num_beam* amount of tokens that have the highest joint probability. Consider the initial token sequence probability tree illustration with *num_beams*=2 (since *num_beams*=1 would be greedy decoding!):\n",
        "\n",
        "![Beam Search](./figs/beam_search.png)\n",
        "\n",
        "- In the first step t=0, **The nice** seems to have the highest probability, so the model chooses **The**\n",
        "- In the next time step t=1, we notice that the joint probability of **The dog has** ($0.4 \\times 0.9 = 0.36$) is higher than **The nice woman**'s ($0.5 \\times 0.4 = 0.2$)\n",
        "\n",
        "$\\implies$ therefore the model eventualy picks the red branch sequence **The dog has**\n",
        "\n",
        "Let's generate what beam search would give for the same input sentence **\"Bella couldn't take it anymore. Edward was gone for 3 months now and\"**. We also do *early_stopping*=True, which means that we stop generating if the token is the end-of-sentence [EOS] token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Pretrained model output:\n",
            "--------------------------------------------------------------------------------\n",
            "Bella couldn't take it anymore. Edward was gone for 3 months now and Bella had no idea what was going on.\n",
            "\n",
            "\"I'm sorry, Bella,\" Edward said.\n",
            "\n",
            "\"I'm sorry,\" Bella said.\n",
            "\n",
            "\"I'm sorry,\" Edward said.\n",
            "\n",
            "\"I'm sorry,\" Bella said.\n",
            "\n",
            "\"I'm sorry,\" Edward said.\n",
            "\n",
            "\"I'm sorry,\" Bella said.\n",
            "\n",
            "\"I'm sorry,\" Edward said.\n",
            "\n",
            "\"\n",
            "--------------------------------------------------------------------------------\n",
            "Story model output:\n",
            "--------------------------------------------------------------------------------\n",
            "<romance> Bella couldn't take it anymore. Edward was gone for 3 months now and Bella couldn't take it anymore. Bella wanted to be with Edward, but she couldn't take it anymore. Bella wanted to be with Edward, but she couldn't take it anymore. Bella wanted to be with Edward, but she couldn't take it anymore. Bella wanted to be with Edward, but she couldn't take it anymore. Bella wanted to be with Edward, but she couldn't take it anymore\n"
          ]
        }
      ],
      "source": [
        "# 1) Tokenize the text with which we condition the generation\n",
        "pretrained_input_ids = pretrained_tokenizer.encode(\"Bella couldn't take it anymore. Edward was gone for 3 months now and\", return_tensors='pt')\n",
        "story_input_ids = story_tokenizer.encode(\"<romance> Bella couldn't take it anymore. Edward was gone for 3 months now and\", return_tensors='pt')\n",
        "\n",
        "# 2) Generate text until the output length (which includes the context length) \n",
        "#    reaches max_length or early stopping\n",
        "beam_pretrained_output = pretrained_model.generate(\n",
        "    pretrained_input_ids, \n",
        "    max_length=100, \n",
        "    num_beams=4, # set num_beams=4, default num_beams=1\n",
        "    early_stopping=True\n",
        ")\n",
        "beam_story_output = story_model.generate(\n",
        "    story_input_ids, \n",
        "    max_length=100, \n",
        "    num_beams=4, # set num_beams=4, default num_beams=1\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "# 3) Decode the output back into readable strings\n",
        "display_outputs(beam_pretrained_output, beam_story_output)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Q:**\n",
        "- **What is slightly different between the greedy search and the beam search outputs?**\n",
        "- **Given the generated output above, what is a serious problem you find about the beam search algorithm?**\n",
        "- **What is a possible solution?**\n",
        "- **Is there still a difference between the pretrained model and the story finetuned model with respect to these pitfalls?**\n",
        "\n",
        "*A: TODO - your answer here!*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A possible solution proposed by [Paulus et al. (2017)](https://arxiv.org/abs/1705.04304) and [Klein et al. (2017)](https://arxiv.org/abs/1701.02810) is to penalize the model for generating a sequence of grams (another word for words) of $n$-grams ($n$ instances of grams). One way is to make sure that no $n$-gram appears $k$ times by manually setting the probability of next words that could create an already seen $n$-gram to 0.\n",
        "\n",
        "Let's try it out by setting *no_repeat_ngram_size*=2:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Pretrained model output:\n",
            "--------------------------------------------------------------------------------\n",
            "Bella couldn't take it anymore. Edward was gone for 3 months now and Bella had no idea what was going on.\n",
            "\n",
            "\"I don't know what to do,\" Bella said. \"I just want to go back to school. I'm not going to be able to take care of my family anymore.\"\n",
            "--------------------------------------------------------------------------------\n",
            "Story model output:\n",
            "--------------------------------------------------------------------------------\n",
            "<romance> Bella couldn't take it anymore. Edward was gone for 3 months now and Bella wanted to go back to her old life with Edward. Bella decided to leave Edward and move back in with her aunt and uncle. But Bella's aunt was not happy with the way Bella was living. She wanted Bella to stay in the house, but Bella didn't want to be a part of the family.  Bella and Edward had a lot of fun together. One day, Bella went to the\n"
          ]
        }
      ],
      "source": [
        "# 1) Tokenize the text with which we condition the generation\n",
        "pretrained_input_ids = pretrained_tokenizer.encode(\"Bella couldn't take it anymore. Edward was gone for 3 months now and\", return_tensors='pt')\n",
        "story_input_ids = story_tokenizer.encode(\"<romance> Bella couldn't take it anymore. Edward was gone for 3 months now and\", return_tensors='pt')\n",
        "\n",
        "# 2) Generate text until the output length (which includes the context length) \n",
        "#    reaches max_length or early stopping\n",
        "beam_pretrained_output = pretrained_model.generate(\n",
        "    pretrained_input_ids, \n",
        "    max_length=100, \n",
        "    num_beams=4, \n",
        "    no_repeat_ngram_size=2, # set no_repeat_ngram_size=2\n",
        "    early_stopping=True\n",
        ")\n",
        "beam_story_output = story_model.generate(\n",
        "    story_input_ids, \n",
        "    max_length=100, \n",
        "    num_beams=4, \n",
        "    no_repeat_ngram_size=2, # set no_repeat_ngram_size=2\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "# 3) Decode the output back into readable strings\n",
        "display_outputs(beam_pretrained_output, beam_story_output)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Yay! It looks like most problematic repetitiveness is avoided. However, note that this an extreme approach. What if S√£o Paulo or New York had to be mentioned in a generated article and we penalized the 2-gram city names? The algorithm won't re-use the bigram, and the text could look unnatural.\n",
        "\n",
        "Another important property of generation is variety. For an application such as a writing aid, we might want to see different generated sentences, and pick the one we like the most. Given that there could be some subsequences that have close probabilities, can we get different outputs from beam-search? :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Pretrained model output:\n",
            "--------------------------------------------------------------------------------\n",
            "1: Bella couldn't take it anymore. Edward was gone for 3 months now and Bella had no idea what was going on.\n",
            "\n",
            "\"I don't know what to do,\" Bella said. \"I just want to go back to school. I'm not going to be able to take care of my family anymore.\"\n",
            "--------------------\n",
            "2: Bella couldn't take it anymore. Edward was gone for 3 months now and Bella had no idea what was going on.\n",
            "\n",
            "\"I don't know what to do,\" Bella said. \"I just want to go back to school. I'm not going to be able to take care of my family. It's been a long time since I've been here, and I can't do anything about it. But I know that if I do, I'll be back in the world.\n",
            "--------------------\n",
            "3: Bella couldn't take it anymore. Edward was gone for 3 months now and Bella had no idea what was going on.\n",
            "\n",
            "\"I don't know what to do,\" Bella said. \"I just want to go back to school. I'm not going to be able to take care of my family. It's been a long time since I've been here, and I can't do anything about it. But I know that if I do, I'll be back in the world.\"\n",
            "--------------------------------------------------------------------------------\n",
            "Story model output:\n",
            "--------------------------------------------------------------------------------\n",
            "1: <romance> Bella couldn't take it anymore. Edward was gone for 3 months now and Bella wanted to go back to her old life with Edward. Bella decided to leave Edward and move back in with her aunt and uncle. But Bella's aunt was not happy with the way Bella was living. She wanted Bella to stay in the house, but Bella didn't want to be a part of the family.  Bella and Edward had a lot of fun together. One day, Bella went to the\n",
            "--------------------\n",
            "2: <romance> Bella couldn't take it anymore. Edward was gone for 3 months now and Bella wanted to go back to her old life with Edward. Bella decided to leave Edward and move back in with her aunt and uncle. But Bella's aunt was not happy with the way Bella was living. She wanted Bella to stay in the house, but Bella didn't want to be a part of the family.  Bella and Edward had a lot of fun together. One day, Bella went to a\n",
            "--------------------\n",
            "3: <romance> Bella couldn't take it anymore. Edward was gone for 3 months now and Bella wanted to go back to her old life with Edward. Bella decided to leave Edward and move back in with her aunt and uncle. But Bella's aunt was not happy with the way Bella was living. She wanted Bella to stay in the house, but Bella didn't want to be a part of the family.  Bella and Edward had a lot of fun together. One day, Bella went to see\n"
          ]
        }
      ],
      "source": [
        "# 1) Tokenize the text with which we condition the generation\n",
        "pretrained_input_ids = pretrained_tokenizer.encode(\"Bella couldn't take it anymore. Edward was gone for 3 months now and\", return_tensors='pt')\n",
        "story_input_ids = story_tokenizer.encode(\"<romance> Bella couldn't take it anymore. Edward was gone for 3 months now and\", return_tensors='pt')\n",
        "\n",
        "# 2) Generate text until the output length (which includes the context length) \n",
        "#    reaches max_length or early stopping\n",
        "beam_pretrained_output = pretrained_model.generate(\n",
        "    pretrained_input_ids, \n",
        "    max_length=100, \n",
        "    num_beams=4, \n",
        "    no_repeat_ngram_size=2, \n",
        "    early_stopping=True,\n",
        "    num_return_sequences=3 # set num_return_sequences = 3\n",
        ")\n",
        "beam_story_output = story_model.generate(\n",
        "    story_input_ids, \n",
        "    max_length=100, \n",
        "    num_beams=4, \n",
        "    no_repeat_ngram_size=2, \n",
        "    early_stopping=True,\n",
        "    num_return_sequences=3 # set num_return_sequences = 3\n",
        ")\n",
        "\n",
        "# 3) Decode the output back into readable strings\n",
        "display_outputs(beam_pretrained_output, beam_story_output, multiple_output=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Q:**\n",
        "- **What is the variation mostly related to?**\n",
        "- **Given the results do you think this algorithm can be applied as a writing aid?**\n",
        "\n",
        "*A: TODO - your answer here!*"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3) Decoding Takeaways\n",
        "\n",
        "As you can see for most generations, the most likely subsequence is unique and decoding algorithms can be too predictable. This is mainly because of two reasons:\n",
        "\n",
        "- **Generation length:** Beam search works when tasks have predictable length (ex: machine translation or summarization) ([Murray et al. 2018](https://arxiv.org/abs/1808.10006), [Yang et al. 2018](https://arxiv.org/abs/1808.09582)). It's not great for open-ended generation where the desired output length can vary (ex: dialog).\n",
        "\n",
        "- **Suprisal effect:** [Ari Holtzman et al. (2019)](https://arxiv.org/abs/1904.09751) show that high quality human language does not follow a distribution of high probability next words. It's desirable for the generated text to be surprising.\n",
        "\n",
        "Therefore, we need to consider stochastic generation algorithms. Will Bella ever get over Edward? Let's sample to see what happens!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Sampling Algorithms\n",
        "\n",
        "### 3.1) Random Sampling\n",
        "\n",
        "Sampling means randomly picking the next word $w_t$ from ‚Äãits conditional probability distribution $P(w_{t}|w_{1:t-1})$. Let's picture what this can be like with the prior probability trees we have seen:\n",
        "\n",
        "![Beam Search](./figs/sampling_search.png)\n",
        "\n",
        "Each dot here represents a timestamp in decoding. To choose the second token **car** we randomly pick a token among [**nice**, **dog**, **car**]. The model then gives next possible tokens' probabilities and the generation algorithm *randomly* chooses **drives**, which happens to be the top-probability token.\n",
        "\n",
        "However, as you can imagine, this may not create very coherent sentences. Let's try it out with GPT-2!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Pretrained model output:\n",
            "--------------------------------------------------------------------------------\n",
            "Bella couldn't take it anymore. Edward was gone for 3 months now and she'd once again turned into a ghost.\n",
            "\n",
            "Jessia tried to escape. Things had changed. Her vocation was free. She tried to stay there but died rather than spend time alone on the tired edge of her own life. Nothing. But now she had to, even though she thought it came down to the nostalgia she had for her old life.\n",
            "\n",
            "When there was a problem she wouldn't have\n",
            "--------------------------------------------------------------------------------\n",
            "Story model output:\n",
            "--------------------------------------------------------------------------------\n",
            "<romance> Bella couldn't take it anymore. Edward was gone for 3 months now and she was seeing a kind of dark older man with aged eyes. Bella don't understand him anymore. Instead of calling out, she picks up the courage to call out to her Uncle Pigeon the man she grew up with and told him the past.And then years and years pass, but Nicky came here,Bella didn't have to stop thinking about him anymore.\n"
          ]
        }
      ],
      "source": [
        "# 1) Tokenize the text with which we condition the generation\n",
        "pretrained_input_ids = pretrained_tokenizer.encode(\"Bella couldn't take it anymore. Edward was gone for 3 months now and\", return_tensors='pt')\n",
        "story_input_ids = story_tokenizer.encode(\"<romance> Bella couldn't take it anymore. Edward was gone for 3 months now and\", return_tensors='pt')\n",
        "\n",
        "# 2) Generate text until the output length (which includes the context length) \n",
        "#    reaches max_length or early stopping\n",
        "# NOTE: setting the seed so the output doesn't always change at every run, \n",
        "#       but you can comment the next line out to see how much it can vary.\n",
        "seed = 84\n",
        "torch.manual_seed(84)\n",
        "sample_pretrained_output = pretrained_model.generate(\n",
        "    pretrained_input_ids, \n",
        "    do_sample=True, \n",
        "    max_length=100, \n",
        "    top_k=0\n",
        ")\n",
        "torch.manual_seed(84)\n",
        "sample_story_output = story_model.generate(\n",
        "    story_input_ids, \n",
        "    do_sample=True, \n",
        "    max_length=100, \n",
        "    top_k=0\n",
        ")\n",
        "\n",
        "# 3) Decode the output back into readable strings\n",
        "display_outputs(sample_pretrained_output, sample_story_output)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As expected the model repeats subsequences much less, but the output is incoherent due to the way we are randomly sampling from the **complete** output distribution."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Q:**\n",
        "- **What are some other possible ways that you can do sampling such that the content isn't completely incoherent?**\n",
        "- **Think about algorithms seen in class. What do you think is best for this problem?**\n",
        "\n",
        "*A: TODO - your answer here!*"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2) Top-$p$ (nucleus) \n",
        "\n",
        "In class, we went over top-$p$ sampling (or also commonly referred to as nucleus sampling). The \"thresholding\" aspect of this algorithm is the cumulative probability mass, which is distributed among the top-$p$ answers that the algorithm can randomly choose from. This way the number of best $k$ tokens dynamically changes according to the model's confidence. Let's see how we can implement it with the *transformers* package. There are 2 types of arguments that we can pass to the model's *generate* function: \n",
        "1. *top_k* : the top $k$ highest probability tokens kept for sampling \n",
        "2. *top_p* : the cumulative probability mass from which the most likely words are sampled, if set to float < 1, only the smallest set of most probable tokens with probabilities that add up to $p$ **or higher** are kept for sampling.\n",
        "    - **Or higher** because if no single token falls under this cumulative probability mass, for example, one token has a probability mass higher than *top_p*, then the default in huggingface is to greedily sample the top-scoring token.\n",
        "    - However, you can imagine setting a minimum *top_k* such that if no token is in this cumulative probability mass, and getting further creative with such edge cases!\n",
        "\n",
        "We set *top_k*=0 to only demonstrate nucleus sampling with *top_p*=0.7."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Pretrained model output:\n",
            "--------------------------------------------------------------------------------\n",
            "Bella couldn't take it anymore. Edward was gone for 3 months now and she'd lost a lot of her feelings.\n",
            "\n",
            "He'd tried to reassure her by saying that he'd had a few encounters with the Ruby, but that was just a tip of the iceberg.\n",
            "\n",
            "\"I don't know how you'd handle this, Edward,\" she said, tilting her head to look at him. \"It's really like a nightmare, Edward.\"\n",
            "\n",
            "\"It's really like\n",
            "--------------------------------------------------------------------------------\n",
            "Story model output:\n",
            "--------------------------------------------------------------------------------\n",
            "<romance> Bella couldn't take it anymore. Edward was gone for 3 months now and she was seeing a new guy. Bella wanted to be with him but she couldn't because she was afraid of him. When Bella is down with her new boyfriend Simon, her mother decides to get her married to a very good guy, Colin. Bella decides to break up with Simon and move away.  However, Bella doesn't know what to do. She decides to stop talking to Colin and decides to\n"
          ]
        }
      ],
      "source": [
        "# 1) Tokenize the text with which we condition the generation\n",
        "pretrained_input_ids = pretrained_tokenizer.encode(\"Bella couldn't take it anymore. Edward was gone for 3 months now and\", return_tensors='pt')\n",
        "story_input_ids = story_tokenizer.encode(\"<romance> Bella couldn't take it anymore. Edward was gone for 3 months now and\", return_tensors='pt')\n",
        "\n",
        "# 2) Generate text until the output length (which includes the context length) \n",
        "#    reaches max_length or early stopping\n",
        "# NOTE: setting the seed so the output doesn't always change at every run, \n",
        "#       but you can comment the next line out to see how much it can vary.\n",
        "seed = 84\n",
        "torch.manual_seed(84)\n",
        "sample_pretrained_output = pretrained_model.generate(\n",
        "    pretrained_input_ids, \n",
        "    do_sample=True, \n",
        "    max_length=100, \n",
        "    top_p=0.7, # sample only from tokens falling in the top cumulative 70% probability\n",
        "    top_k=0 # deactivate top_k sampling\n",
        ")\n",
        "torch.manual_seed(84)\n",
        "sample_story_output = story_model.generate(\n",
        "    story_input_ids, \n",
        "    do_sample=True, \n",
        "    max_length=100, \n",
        "    top_p=0.7, # sample only from tokens falling in the top cumulative 70% probability\n",
        "    top_k=0 # deactivate top_k sampling\n",
        ")\n",
        "\n",
        "# 3) Decode the output back into readable strings\n",
        "display_outputs(sample_pretrained_output, sample_story_output)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The coherence is somewhat better with respect to the context given! But what about variance?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Pretrained model output:\n",
            "--------------------------------------------------------------------------------\n",
            "1: Bella couldn't take it anymore. Edward was gone for 3 months now and she'd lost a lot of her feelings.\n",
            "\n",
            "He'd tried to reassure her by saying that he'd had a few encounters with the Ruby, but that was just a tip of the iceberg.\n",
            "\n",
            "\"I don't know how you'd handle this, Edward,\" she said, tilting her head to look at him. \"It's really like a nightmare, Edward.\"\n",
            "\n",
            "\"It's really like\n",
            "--------------------\n",
            "2: Bella couldn't take it anymore. Edward was gone for 3 months now and he wanted to know how he was going to do things for her.\n",
            "\n",
            "So she asked her dad for his help in finding Edward. They agreed and he told Bella how to keep her safe.\n",
            "\n",
            "He told her how to get\"))esa home to stop going to the vet and have his wife return to her.\n",
            "\n",
            "She told him that Edward had already been kidnapped and that he had to go home\n",
            "--------------------\n",
            "3: Bella couldn't take it anymore. Edward was gone for 3 months now and Elsa was feeling awful, at least in the short term. Bella was starting to show her promise in the playground, but Elsa was probably just going to ignore her. She needed to stop acting like a single, motherly figure, just by doing what she was supposed to do. Bella had a motherly sense of what was best for her, and just wanted to keep Elsa safe. Bella didn't want her to be\n",
            "--------------------------------------------------------------------------------\n",
            "Story model output:\n",
            "--------------------------------------------------------------------------------\n",
            "1: <romance> Bella couldn't take it anymore. Edward was gone for 3 months now and she was seeing a new guy. Bella wanted to be with him but she couldn't because she was afraid of him. When Bella is down with her new boyfriend Simon, her mother decides to get her married to a very good guy, Colin. Bella decides to break up with Simon and move away.  However, Bella doesn't know what to do. She decides to stop talking to Colin and decides to\n",
            "--------------------\n",
            "2: <romance> Bella couldn't take it anymore. Edward was gone for 3 months now and Bella didn't want to live with him anymore.  So she had to get out of the house. She had to start planning her life, like she always did.  Bella must do her best to make sure she's not seen her recompense of their life. She must learn to take care of herself and make herself responsible.  Bella has to look after her daughter, Edward, and her daughter\n",
            "--------------------\n",
            "3: <romance> Bella couldn't take it anymore. Edward was gone for 3 months now and he was telling lies to Bella. Bella knew about it but it was only a fantasy. Bella was afraid that Edward would harm her anymore.  When Bella went to sleep, Edward wanted to kill her. He sliced her throat. Then, Bella told him about how he made her cry and tortured her. He was supposed to tell her to kill him and his master but instead, he killed her. \n"
          ]
        }
      ],
      "source": [
        "# 1) Tokenize the text with which we condition the generation\n",
        "pretrained_input_ids = pretrained_tokenizer.encode(\"Bella couldn't take it anymore. Edward was gone for 3 months now and\", return_tensors='pt')\n",
        "story_input_ids = story_tokenizer.encode(\"<romance> Bella couldn't take it anymore. Edward was gone for 3 months now and\", return_tensors='pt')\n",
        "\n",
        "# 2) Generate text until the output length (which includes the context length) \n",
        "#    reaches max_length or early stopping\n",
        "# NOTE: setting the seed so the output doesn't always change at every run, \n",
        "#       but you can comment the next line out to see how much it can vary.\n",
        "seed = 84\n",
        "torch.manual_seed(84)\n",
        "sample_pretrained_output = pretrained_model.generate(\n",
        "    pretrained_input_ids, \n",
        "    do_sample=True, \n",
        "    max_length=100, \n",
        "    top_p=0.7, # sample only from tokens falling in the top cumulative 70% probability\n",
        "    top_k=0, # deactivate top_k sampling\n",
        "    num_return_sequences=3 # set num_return_sequences = 3\n",
        ")\n",
        "torch.manual_seed(84)\n",
        "sample_story_output = story_model.generate(\n",
        "    story_input_ids, \n",
        "    do_sample=True, \n",
        "    max_length=100, \n",
        "    top_p=0.7, # sample only from tokens falling in the top cumulative 70% probability\n",
        "    top_k=0, # deactivate top_k sampling\n",
        "    num_return_sequences=3 # set num_return_sequences = 3\n",
        ")\n",
        "\n",
        "# 3) Decode the output back into readable strings\n",
        "display_outputs(sample_pretrained_output, sample_story_output, multiple_output=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we somewhat expected, the outputs are much more diverse than those of deterministic decoding algorithms. And it looks like Bella does not get a happy ending if *num_return_sequences*=3 ü•≤ even if all three endings are unique in their own way."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3) Sampling Takeaways\n",
        "\n",
        "Sampling can be really finicky according to the input text we condition our generation with and the top-$k$ or top-$p$ hyperparameter options we choose. \n",
        "\n",
        "However, it offers a wider variety of outputs rather than decoding algorithms like greedy and beam search."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Automatic NLG Evaluation Metrics - the curious case of BLEU\n",
        "\n",
        "### Motivation\n",
        "In this section, we will take a close look on how to evaluate generated text quality. In translation or other sequence-to-sequence tasks, we do not have a single way to get a *correct* answer. For example, consider a model that outputs the text:\n",
        "\n",
        "> The cat is fluffly.\n",
        "\n",
        "and the correct *reference* text we have is:\n",
        "\n",
        "> The cat has fluffy hair.\n",
        "\n",
        "These two sentences look like paraphases but are not exact matches. Therefore these tasks require a more flexible evaluation in which we check for matching subsequences of $n$-grams. For this reason, we will take a close look to the popular BLEU score (BiLingual Evaluation Understudy).\n",
        "\n",
        "### What is BLEU?\n",
        "BLEU is a metric proposed by [Papineni et al (2002)](https://aclanthology.org/P02-1040.pdf) for automatic evaluation of machine translation. The BLEU score is number between 0 and 1 that compares the similarity of a predicted *candidate* text to a set of correct *reference* texts, where 1 means that the generated text is close to the reference and is of \"high quality\" and 0 is the opposite. BLEU was originally used for machine translation but can also be applied for any text generation task (ex: summarization). Note that often in practice the BLEU score is reported on a scale of 1 to 100 for granularity, but it's not an accuracy metric!\n",
        "\n",
        "So how do we calculate BLEU? In short, it has two terms:\n",
        "\n",
        "1. **$n$-gram overlap calculation:** BLEU calculates the n-gram overlap counts of how many *candidate* unigrams, bigrams, trigrams, and four-grams ($n$=1,...,4) match their $n$-gram counterpart in the *reference* sequence. **This can therefore be considered as a precision metric.** While unigrams might be capturing lexical level similarity, we can imagine that longer $n$-gram matching captures higher-level syntactic constituent similarity.\n",
        "\n",
        "2. **brevity penalty:** Since BLEU doesn't capture recall with the overlap calculation, we augment it with a brevity penalty. This penalizes generations that are too short compared to a reference length.\n",
        "\n",
        "For more details and an exact equation, checkout Google Cloud Translation's [evaluation documentation](https://cloud.google.com/translate/automl/docs/evaluate#:~:text=BLEU%20(BiLingual%20Evaluation%20Understudy)%20is,of%20high%20quality%20reference%20translations.).\n",
        "\n",
        "Despite being automatic and easy to use, BLEU has many problems with it, depending on where and how it's used. Let's find out what issues there may with it!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1) Pitfall 1: Accurate paraphrases\n",
        "\n",
        "BLEU measures gram-by-gram similarity between the *candidate* and the *reference* text. This means that, correct paraphrases can hurt the score. Let's see how this might happen when we want to translate:\n",
        "- input: **Le chat s'est assis sur le tapis**\n",
        "- candidate/prediction/output: **The feline layed over the rug**\n",
        "- reference: **The cat sat on the mat**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'bleu': 0.0,\n",
              " 'precisions': [0.3333333333333333, 0.0, 0.0, 0.0],\n",
              " 'brevity_penalty': 1.0,\n",
              " 'length_ratio': 1.0,\n",
              " 'translation_length': 6,\n",
              " 'reference_length': 6}"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# NOTE: the huggingface evaluate package provides a BLEU score that returns \n",
        "#       precision at all 4 levels and how much brevity penalty was applied\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "\n",
        "# 1) Accurate paraphrase\n",
        "predictions = [\"the feline layed over the rug\"]\n",
        "references = [\n",
        "    [\"the cat sat on the mat\"],\n",
        "]\n",
        "results = bleu.compute(predictions=predictions, references=references)\n",
        "results"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see while the paraphased sentence is close to the reference, it gets only a slight score on BLEU-1 (leftmost in precisions), and none of BLEU-4 (rightmost in precisions) as the four-gram overlap is none. On the other hand, let's take a look at a semantically incorrect but bag-of-words-wise close sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'bleu': 0.0, 'precisions': [1.0, 0.5, 0.0, 0.0], 'brevity_penalty': 0.36787944117144233, 'length_ratio': 0.5, 'translation_length': 3, 'reference_length': 6}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'bleu': 0.0,\n",
              " 'precisions': [1.0, 0.8, 0.25, 0.0],\n",
              " 'brevity_penalty': 1.0,\n",
              " 'length_ratio': 1.0,\n",
              " 'translation_length': 6,\n",
              " 'reference_length': 6}"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 2) Removed stopwords\n",
        "predictions = [\"cat sat mat\"]\n",
        "references = [\n",
        "    [\"the cat sat on the mat\"],\n",
        "]\n",
        "results = bleu.compute(predictions=predictions, references=references)\n",
        "print(results)\n",
        "\n",
        "# 3) switched \"mat\" and \"cat\"\n",
        "predictions = [\"the mat sat on the cat\"]\n",
        "references = [\n",
        "    [\"the cat sat on the mat\"],\n",
        "]\n",
        "results = bleu.compute(predictions=predictions, references=references)\n",
        "results"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Every gram that is in the reference text is also in the prediction. Therefore, BLEU-1 gets full score and BLEU-3 gets a reasonable precision on these examples. However, due to the fact that there are no four-gram overlaps, the overall BLEU score is 0.0."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2) Pitfall 2: Senseless repetitions"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Are there any ways we could get the four-gram overlap without coherent mapping among the prediction and the reference?\n",
        "\n",
        "- input: **Le chat s'est assis sur le tapis**\n",
        "- prediction: \n",
        "    - **The cat sat on the cat, is on the cat** \n",
        "    - or **The cat sat on the cat, the mat sat on the mat**\n",
        "- reference: **The cat sat on the mat**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'bleu': 0.35084396956386854, 'precisions': [0.45454545454545453, 0.4, 0.3333333333333333, 0.25], 'brevity_penalty': 1.0, 'length_ratio': 1.8333333333333333, 'translation_length': 11, 'reference_length': 6}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'bleu': 0.38058030016749456,\n",
              " 'precisions': [0.46153846153846156,\n",
              "  0.4166666666666667,\n",
              "  0.36363636363636365,\n",
              "  0.3],\n",
              " 'brevity_penalty': 1.0,\n",
              " 'length_ratio': 2.1666666666666665,\n",
              " 'translation_length': 13,\n",
              " 'reference_length': 6}"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions = [\"the cat sat on the cat, is on the cat\"]\n",
        "references = [\n",
        "    [\"the cat sat on the mat\"],\n",
        "]\n",
        "results = bleu.compute(predictions=predictions, references=references)\n",
        "print(results)\n",
        "\n",
        "predictions = [\"the cat sat on the cat, the mat sat on the mat\"]\n",
        "references = [\n",
        "    [\"the cat sat on the mat\"],\n",
        "]\n",
        "results = bleu.compute(predictions=predictions, references=references)\n",
        "results"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now this is the first time we got a BLEU score that is above 0. As you can see, none of the translations actually make sense, it's simply because we can find a matching four-gram like \"the cat sat on\" ... \"sat on the mat\", and other possibilities. Despite all grams showing up, the BLEU-1 score is reduced due to a lack of brevity."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3) Other problems with BLEU\n",
        "\n",
        "There are some important properties of BLEU that we need to consider when deciding to use it. For example:\n",
        "1. BLEU performs particularly bad when tested on individual *reference* sentences, or when the length of the generation isn't consistent across a corpus.\n",
        "2. At least one matching 4-gram is required to get a BLEU score > 0.\n",
        "3. This score is also very dependent on the sort of tokenization used to get the $n$-grams."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9thTgP1AKYii"
      },
      "source": [
        "## 5) Conclusion\n",
        "\n",
        "We found out that greedy and beam search often result in repetitive patterns where the generation loops on subsequences.\n",
        "However, there are penalization and sampling approaches that help us get coherent generation without the repetitive subsequences.\n",
        "\n",
        "We also went over a popular NLG evaluation metric and saw how tough it can be to evaluate a task where the output is allowed to be diverse sequences. \n",
        "While BLEU is one option, there are many more metrics out there. Often in practise we have to use all metrics at the same time to have the big picture. \n",
        "Otherwise, as we have seen with BLEU, every metric has its own shortage and cannot give us a good understanding on how our model is behaving compared to a given large corpus of high quality references.\n",
        "\n",
        "### What else can I do?\n",
        "\n",
        "Checkout the huggingface [transformers text generation documentation](https://huggingface.co/docs/transformers/main_classes/text_generation) to see what other models you can generate with. There are many more sampling approaches being offered these days, and is a hot topic in NLP. If you want to read a bit more on new methods, checkout this paper on [Locally Typical Sampling](https://arxiv.org/abs/2202.00666) (a generation option that is now available with the *transformers* `generate` function).\n",
        "\n",
        "You can also browse through their selection of [metrics](https://huggingface.co/docs/datasets/how_to_metrics#compute-scores) or the torch package's [metrics](https://torchmetrics.readthedocs.io/en/stable/all-metrics.html#) to find more NLG metrics."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.15 ('inlpvenv')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "fc2d99b9d17c3b8326e5aaa51e8ab768ef640c8dd496033039049585146abff1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
